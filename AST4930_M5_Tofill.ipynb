{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# AST 4930 Module 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emsemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "X, y = datasets.make_moons(n_samples=200, noise=0.3, random_state=3)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr')\n",
    "\n",
    "ax.set_xlabel(\"Feature 1\")\n",
    "ax.set_ylabel(\"Feature 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's build a Decision Tree and check the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_decision_boundary(model, X, y, alpha=0.5, contour=True, marker=None):\n",
    "    xx = np.linspace(X[:, 0].min() - 0.5, X[:, 0].max() + 0.5, 100)\n",
    "    yy = np.linspace(X[:, 1].min() - 0.5, X[:, 1].max() + 0.5, 100)\n",
    "\n",
    "    XX, YY = np.meshgrid(xx, yy)\n",
    "    Z = model.predict(np.c_[XX.ravel(), YY.ravel()])\n",
    "    Z = Z.reshape(XX.shape)\n",
    "\n",
    "    if contour:\n",
    "        plt.contourf(XX, YY, Z, alpha=alpha, cmap='bwr')\n",
    "    \n",
    "    if marker == None:\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=y, s=10, cmap='bwr')\n",
    "    else:\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=y, s=10, cmap='bwr', marker=marker)\n",
    "        \n",
    "    plt.xlabel('feature 1')\n",
    "    plt.ylabel('feature 2')\n",
    "    \n",
    "plot_decision_boundary(model, X_train, y_train)\n",
    "plot_decision_boundary(model, X_test, y_test, contour=False, marker='^')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "npanel = 12\n",
    "\n",
    "fig, axs = plt.subplots(4, 3, figsize=(12, 12))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i in np.arange(npanel):\n",
    "    # This is where bootstrap is happening.\n",
    "    indices_with_replacement = np.random.randint(0, len(X), len(X)) # from 0 to len(X)-1, draw len(X) samples.\n",
    "    \n",
    "    X_new = X[indices_with_replacement]\n",
    "    y_new = y[indices_with_replacement]\n",
    "\n",
    "    axs[i].scatter(X_new[:, 0], X_new[:, 1], c=y_new, s=10, cmap='bwr')\n",
    "    axs[i].set_xlim(-1.5,2.5)\n",
    "    axs[i].set_ylim(-1.5,2.)\n",
    "    \n",
    "plt.savefig('bootstrap.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(np.random.randint(0, len(X), len(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "npanel = 12\n",
    "\n",
    "fig, axs = plt.subplots(4, 3, figsize=(12, 12))\n",
    "axs = axs.ravel()\n",
    "\n",
    "xx = np.linspace(X[:, 0].min() - 0.5, X[:, 0].max() + 0.5, 100)\n",
    "yy = np.linspace(X[:, 1].min() - 0.5, X[:, 1].max() + 0.5, 100)\n",
    "\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "\n",
    "for i in np.arange(npanel):\n",
    "    indices_with_replacement = np.random.randint(0, len(X), len(X))\n",
    "    X_new = X[indices_with_replacement]\n",
    "    y_new = y[indices_with_replacement]\n",
    "    # DT model with bootstrapped data\n",
    "    model = DecisionTreeClassifier(random_state=i)\n",
    "    model.fit(X_new, y_new)\n",
    "    Z = model.predict(np.c_[XX.ravel(), YY.ravel()])\n",
    "    Z = Z.reshape(XX.shape)\n",
    "\n",
    "    axs[i].contourf(XX, YY, Z, alpha=0.5, cmap='bwr')\n",
    "    axs[i].scatter(X_new[:, 0], X_new[:, 1], c=y_new, s=10, cmap='bwr')\n",
    "\n",
    "plt.savefig('bootstrap_db.png',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's increase the number of sample to 1000 and compute a mean decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "npanel = 1000\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "axs = axs.ravel()\n",
    "\n",
    "xx = np.linspace(X[:, 0].min() - 0.5, X[:, 0].max() + 0.5, 100)\n",
    "yy = np.linspace(X[:, 1].min() - 0.5, X[:, 1].max() + 0.5, 100)\n",
    "\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "\n",
    "# Array to save the averaged decision boundary\n",
    "Zavg = np.zeros_like(XX)\n",
    "\n",
    "for i in np.arange(npanel-1):\n",
    "    indices_with_replacement = np.random.randint(0, len(X), len(X))\n",
    "    X_new = X[indices_with_replacement]\n",
    "    y_new = y[indices_with_replacement]\n",
    "    # DT model with bootstrapped data\n",
    "    model = DecisionTreeClassifier(random_state=i)\n",
    "    model.fit(X_new, y_new)\n",
    "    Z = model.predict(np.c_[XX.ravel(), YY.ravel()])\n",
    "    Z = Z.reshape(XX.shape)\n",
    "    Zavg += Z\n",
    "\n",
    "    \n",
    "# Average the decision boundary\n",
    "Zavg /= npanel\n",
    "\n",
    "Zavg_ = Zavg.copy()\n",
    "Zavg_[Zavg <= 0.5] = 0\n",
    "Zavg_[Zavg > 0.5] = 1\n",
    "\n",
    "axs[0].contourf(XX, YY, Zavg, alpha=0.5, cmap='bwr')\n",
    "axs[0].scatter(X[:, 0], X[:, 1], c=y, s=10, cmap='bwr')\n",
    "axs[0].set_title('mean decision boundary')\n",
    "\n",
    "axs[1].contourf(XX, YY, Zavg_, alpha=0.5, cmap='bwr')\n",
    "axs[1].scatter(X[:, 0], X[:, 1], c=y, s=10, cmap='bwr')\n",
    "axs[1].set_title('mean decision boundary (binary classification)')\n",
    "\n",
    "plt.savefig('mean_db.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "# See https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=1000, \n",
    "                            max_samples=1.0, bootstrap=True, random_state=0)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "tree_clf.fit(X_train, y_train)\n",
    "tree_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, axes = plt.subplots(ncols=2, figsize=(10,4), sharey=True)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plot_decision_boundary(tree_clf, X, y)\n",
    "plt.title(\"A Single Decision Tree\", fontsize=14)\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plot_decision_boundary(bag_clf, X, y)\n",
    "plt.title(\"Decision Trees with Bagging\", fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When we use a Decision Tree as our base estimator, we can use RandomForestClaasifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#See https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=1000, oob_score=True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "plot_decision_boundary(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two ways to check the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=1000, oob_score=True)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=1000, oob_score=True)\n",
    "model.fit(X, y)\n",
    "model.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try Random Forests on the [MNIST data](https://en.wikipedia.org/wiki/MNIST_database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to select Tensorflow kernel to load the data\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load the data, split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  We will take 1/50 of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[::50]\n",
    "y_train = y_train[::50]\n",
    "X_test = X_test[::50]\n",
    "y_test = y_test[::50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(X_train[0], cmap='gray')\n",
    "plt.title('The number is = {}'.format(y_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will \"flatten\" the data, by converting a 2d array with 28x28 elements to an 1d array with 784 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2])\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the performance of a single DT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Write a script that finds the DT model using hyperparameter optimization and check the test score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's compare the model prediction with the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Prediction: {}\".format(grid_search.best_estimator_.predict(X_test[:10])))\n",
    "print(\"Ground truth: {}\".format(y_test[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize some of the incorrect predictions to see if they make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "plt.imshow(X_test[n].reshape(28,28))\n",
    "plt.title('The model prediction is = {}'.format(grid_search.best_estimator_.predict(np.expand_dims(X_test[n],axis=0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Now let's check out how Random Forests perform. Compare the score obtained with test dataset and the OOB score obtained with the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Let's compare the model prediction with the ground truth. How does the performance compared with a single DT you trained above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Can you visualize feature importance in two dimensions (i.e., 28x28 image)? What does this image mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "X, y = datasets.make_moons(n_samples=500, noise=0.3, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr')\n",
    "\n",
    "ax.set_xlabel(\"Feature 1\")\n",
    "ax.set_ylabel(\"Feature 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "n_estimators = 1000\n",
    "\n",
    "model = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1), \n",
    "                           n_estimators=n_estimators,\n",
    "                           learning_rate=0.5, random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))\n",
    "\n",
    "plot_decision_boundary(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pred in model.staged_predict(X_train):\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score in model.staged_score(X_train,y_train):\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/modules/generated/sklearn.metrics.zero_one_loss.html\n",
    "from sklearn.metrics import zero_one_loss\n",
    "\n",
    "err_train = np.zeros((n_estimators))\n",
    "score_train = np.zeros((n_estimators))\n",
    "\n",
    "for i, y_pred in enumerate(model.staged_predict(X_train)):\n",
    "    err_train[i] = zero_one_loss(y_pred, y_train)\n",
    "\n",
    "for i, score in enumerate(model.staged_score(X_train,y_train)):\n",
    "    score_train[i] = score\n",
    "\n",
    "err_test = np.zeros((n_estimators))\n",
    "score_test = np.zeros((n_estimators))\n",
    "\n",
    "for i, y_pred in enumerate(model.staged_predict(X_test)):\n",
    "    err_test[i] = zero_one_loss(y_pred, y_test)\n",
    "\n",
    "for i, score in enumerate(model.staged_score(X_test,y_test)):\n",
    "    score_test[i] = score\n",
    "    \n",
    "    \n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(score_train, label='train score')\n",
    "ax1.plot(score_test, label='test score')\n",
    "ax1.set_xlabel('number of boosting')\n",
    "ax1.set_ylabel('accuracy')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(err_train, label='train error')\n",
    "ax2.plot(err_test, label='test error')\n",
    "ax2.set_xlabel('number of boosting')\n",
    "ax2.set_ylabel('error')\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Repeat the previous experiment using learning_rate=0.1 and learning_rate=5.0. What do the results tell you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try AdaBoost on two Gaussians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Construct dataset\n",
    "X1, y1 = make_gaussian_quantiles(cov=2., n_samples=200, n_features=2, \n",
    "                                 n_classes=2, random_state=1)\n",
    "X2, y2 = make_gaussian_quantiles(mean=(3, 3), cov=1.5, n_samples=300, n_features=2, \n",
    "                                 n_classes=2, random_state=1)\n",
    "\n",
    "# Merge the two Gaussian\n",
    "X = np.concatenate((X1, X2))\n",
    "# We reverse the class of y2\n",
    "y = np.concatenate((y1, - y2 + 1))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "#mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=10, cmap='bwr')\n",
    "\n",
    "ax.set_xlabel(\"Feature 1\")\n",
    "ax.set_ylabel(\"Feature 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What ML algorithm do you think would work the best? SVM!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Find the best SVM model using hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Plot the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM did a good job as we expected. What about DT?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Find the best (single) DT model using hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Plot the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single DT did okay. Now, let's check out DT with AdaBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want a poor base estimator, so we will use max_depth = 1\n",
    "model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), \n",
    "                           algorithm=\"SAMME\", \n",
    "                           n_estimators=200, random_state=0)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# Plot the decision boundaries\n",
    "plot_decision_boundary(model, X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization for AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'estimator__max_depth': [3, 5, 10],\n",
    "              'n_estimators': [10, 50, 100, 200],\n",
    "              'learning_rate': [0.001, 0.01, 0.1, 1.0]}\n",
    "\n",
    "grid_search = GridSearchCV(AdaBoostClassifier(estimator=DecisionTreeClassifier(), \n",
    "                                              algorithm=\"SAMME\", random_state=0), \n",
    "                           param_grid, cv=5, return_train_score=True, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best model: {}\".format(grid_search.best_estimator_))\n",
    "print(\"Test score: {:.2f}\".format(grid_search.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# Plot the decision boundaries\n",
    "plot_decision_boundary(grid_search.best_estimator_, X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem with outliers: we will use exactly the same two Gaussian dataset, but with an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct dataset\n",
    "X1, y1 = make_gaussian_quantiles(cov=2., n_samples=200, n_features=2, \n",
    "                                 n_classes=2, random_state=1)\n",
    "X2, y2 = make_gaussian_quantiles(mean=(3, 3), cov=1.5, n_samples=300, n_features=2, \n",
    "                                 n_classes=2, random_state=1)\n",
    "\n",
    "# Merge the two Gaussian\n",
    "X = np.concatenate((X1, X2))\n",
    "# We reverse the class of y2\n",
    "y = np.concatenate((y1, - y2 + 1))\n",
    "\n",
    "# These are the outliers.\n",
    "X_out = np.array([[0, 0], [0.1, 0.1], [-0.1, -0.1]])\n",
    "y_out = np.array([1, 1, 1])\n",
    "\n",
    "X = np.concatenate((X, X_out))\n",
    "y = np.concatenate((y, y_out))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=10, cmap='bwr')\n",
    "\n",
    "ax.set_xlabel(\"Feature 1\")\n",
    "ax.set_ylabel(\"Feature 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want a \"weak\" learner\n",
    "model = AdaBoostClassifier(DecisionTreeClassifier(max_depth=10), algorithm=\"SAMME\", \n",
    "                           n_estimators=300, random_state=0)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# Plot the decision boundaries\n",
    "plot_decision_boundary(model, X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about Random Forests? Does RF have the same problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Build a RF model and make a plot showing the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)\n",
    "\n",
    "plt.plot(X, y, 'b.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a GBR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor(max_depth=2, n_estimators=20, \n",
    "                                  learning_rate=1.0, random_state=42)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(regressors, X, y, axes, label=None, style=\"r-\", data_style=\"b.\", data_label=None):\n",
    "    x1 = np.linspace(axes[0], axes[1], 500)\n",
    "    y_pred = sum(regressor.predict(x1.reshape(-1, 1)) for regressor in regressors)\n",
    "    plt.plot(X[:, 0], y, data_style, label=data_label)\n",
    "    plt.plot(x1, y_pred, style, linewidth=2, label=label)\n",
    "    if label or data_label:\n",
    "        plt.legend(loc=\"upper center\", fontsize=16)\n",
    "    plt.axis(axes)\n",
    "    \n",
    "fix, ax = plt.subplots(figsize=(6,4))\n",
    "\n",
    "plot_predictions([model], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"Ensemble predictions\")\n",
    "\n",
    "plt.title(\"learning_rate={}, n_estimators={}\".format(model.learning_rate, model.n_estimators), fontsize=14)\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 2\n",
    "n_estimators = 30\n",
    "\n",
    "model = GradientBoostingRegressor(max_depth=max_depth, n_estimators=n_estimators, \n",
    "                                  learning_rate=1, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "model_slow = GradientBoostingRegressor(max_depth=max_depth, n_estimators=n_estimators, \n",
    "                                       learning_rate=0.1, random_state=42)\n",
    "model_slow.fit(X, y)\n",
    "\n",
    "fix, axes = plt.subplots(ncols=2, figsize=(10,4), sharey=True)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plot_predictions([model], X, y, axes=[-0.5, 0.5, -0.1, 0.8])\n",
    "plt.title(\"learning_rate={}, n_estimators={}\".format(model.learning_rate, model.n_estimators), fontsize=14)\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plot_predictions([model_slow], X, y, axes=[-0.5, 0.5, -0.1, 0.8])\n",
    "plt.title(\"learning_rate={}, n_estimators={}\".format(model_slow.learning_rate, model_slow.n_estimators), fontsize=14)\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting with Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=49)\n",
    "\n",
    "model = GradientBoostingRegressor(max_depth=2, n_estimators=120, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Here, \"staged_predict\" gives the prediction at each stage.\n",
    "errors = [mean_squared_error(y_test, y_pred) for y_pred in model.staged_predict(X_test)]\n",
    "best_n_estimators = np.argmin(errors) + 1\n",
    "\n",
    "model_best = GradientBoostingRegressor(max_depth=2, n_estimators=best_n_estimators, random_state=42)\n",
    "model_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "min_error = np.min(errors)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(errors, \"b.-\")\n",
    "plt.plot([best_n_estimators, best_n_estimators], [0, min_error], \"k--\")\n",
    "plt.plot([0, 120], [min_error, min_error], \"k--\")\n",
    "plt.plot(best_n_estimators, min_error, \"ko\")\n",
    "plt.text(best_n_estimators, min_error*1.2, \"Minimum\", ha=\"center\", fontsize=14)\n",
    "plt.axis([0, 120, 0, 0.01])\n",
    "plt.xlabel(\"Number of trees\")\n",
    "plt.ylabel(\"Error\", fontsize=16)\n",
    "plt.title(\"Validation error\", fontsize=14)\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_predictions([model_best], X, y, axes=[-0.5, 0.5, -0.1, 0.8])\n",
    "plt.title(\"Best model (%d trees)\" % best_n_estimators, fontsize=14)\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Should we run all the models to find the best model? We can stop early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#When warm_start is set to True, reuse the solution of the previous call to fit \n",
    "#and add more estimators to the ensemble, otherwise, just erase the previous solution.\n",
    "\n",
    "#See https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#\n",
    "#See also https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_early_stopping.html\n",
    "    \n",
    "model = GradientBoostingRegressor(max_depth=2, warm_start=True, random_state=42)\n",
    "\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "error_tolerance = 5\n",
    "\n",
    "for n_estimators in range(1, 120):\n",
    "    model.n_estimators = n_estimators\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    val_error = mean_squared_error(y_test, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == error_tolerance:\n",
    "            model.n_estimators = n_estimators - error_tolerance\n",
    "            break  # early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.n_estimators)\n",
    "print(\"Minimum validation MSE:\", min_val_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best = GradientBoostingRegressor(max_depth=2, n_estimators=model.n_estimators, random_state=42)\n",
    "model_best.fit(X_train, y_train)\n",
    "\n",
    "plot_predictions([model_best], X, y, axes=[-0.5, 0.5, -0.1, 0.8])\n",
    "plt.title(\"Best model (%d trees)\" % model.n_estimators, fontsize=14)\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
